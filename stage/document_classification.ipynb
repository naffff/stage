{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSzKBCXKHbEB"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -q https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb\n",
        "cp wkhtmltox_0.12.6-1.bionic_amd64.deb /usr/bin\n",
        "apt -qq install /usr/bin/wkhtmltox_0.12.6-1.bionic_amd64.deb\n",
        "sudo apt-get install wkhtmltopdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pcIqkDSHgfJ"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers==4.27.2 --progress-bar off\n",
        "!pip install -qqq pytorch-lightning==1.9.4 --progress-bar off\n",
        "!pip install -qqq torchmetrics==0.11.4 --progress-bar off\n",
        "!pip install -qqq imgkit==1.2.3 --progress-bar off\n",
        "!pip install -qqq easyocr==1.6.2 --progress-bar off\n",
        "!pip install -qqq Pillow==9.4.0 --progress-bar off\n",
        "!pip install -qqq tensorboardX==2.5.1 --progress-bar off\n",
        "!pip install -qqq huggingface_hub==0.11.1 --progress-bar off\n",
        "!pip install -qqq --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1ifQELHv9Z"
      },
      "outputs": [],
      "source": [
        "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imgkit\n",
        "import easyocr\n",
        "import torchvision.transforms as T\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from typing import List\n",
        "import json\n",
        "from torchmetrics import Accuracy\n",
        "from huggingface_hub import notebook_login\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "%matplotlib inline\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfyLjNvLH83S",
        "outputId": "6bd59b9b-5c6d-42e8-cd98-6b23e83ef00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tMZXonmajLPK9zhZ2dt-CdzRTs5YfHy0\n",
            "To: /content/financial-documents.zip\n",
            "\r  0% 0.00/3.10M [00:00<?, ?B/s]\r 17% 524k/3.10M [00:00<00:00, 5.22MB/s]\r100% 3.10M/3.10M [00:00<00:00, 18.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1tMZXonmajLPK9zhZ2dt-CdzRTs5YfHy0\n",
        "!unzip -q financial-documents.zip\n",
        "!mv \"TableClassifierQuaterlyWithNotes\" \"documents\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwZbN7dtIVPF",
        "outputId": "ff8fe1fb-5192-4567-9eed-cae8d07d5324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('documents/income_statement'),\n",
              " PosixPath('documents/cash_flow'),\n",
              " PosixPath('documents/notes'),\n",
              " PosixPath('documents/balance_sheets'),\n",
              " PosixPath('documents/others')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for dir in Path(\"documents\").glob(\"*\"):\n",
        "  dir.rename(str(dir).lower().replace(\" \", \"_\"))\n",
        "\n",
        "list(Path(\"documents\").glob(\"*\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSgxbBlZIXj9"
      },
      "outputs": [],
      "source": [
        "for dir in Path(\"documents\").glob(\"*\"):\n",
        "    image_dir = Path(f\"images/{dir.name}\")\n",
        "    image_dir.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQVqxCkTIdtR"
      },
      "outputs": [],
      "source": [
        "def convert_html_to_image(file_path: Path, images_dir: Path, scale: float = 1.0) -> Path:\n",
        "    file_name = file_path.with_suffix(\".jpg\").name\n",
        "    save_path = images_dir / file_path.parent.name / f\"{file_name}\"\n",
        "    imgkit.from_file(str(file_path), save_path, options={'quiet': '', 'format': 'jpeg'})\n",
        "\n",
        "    image = Image.open(save_path)\n",
        "    width, height = image.size\n",
        "    image = image.resize((int(width * scale), int(height * scale)))\n",
        "    image.save(str(save_path))\n",
        "\n",
        "    return save_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucZWOrQtIfp0"
      },
      "outputs": [],
      "source": [
        "document_paths = list(Path(\"documents\").glob(\"*/*\"))\n",
        "\n",
        "for doc_path in tqdm(document_paths):\n",
        "    convert_html_to_image(doc_path, Path(\"images\"), scale=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqXNapI8M41p"
      },
      "outputs": [],
      "source": [
        "image_paths = sorted(list(Path(\"images\").glob(\"*/*.jpg\")))\n",
        "\n",
        "image = Image.open(image_paths[0]).convert(\"RGB\")\n",
        "width, height = image.size\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBrVtcMeNA0S"
      },
      "outputs": [],
      "source": [
        "reader = easyocr.Reader(['en'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOZ9Nb8NNDwG"
      },
      "outputs": [],
      "source": [
        "image_path = image_paths[0]\n",
        "ocr_result = reader.readtext(str(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0W4H8INJ9z"
      },
      "outputs": [],
      "source": [
        "def create_bounding_box(bbox_data):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for x, y in bbox_data:\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    left = int(min(xs))\n",
        "    top = int(min(ys))\n",
        "    right = int(max(xs))\n",
        "    bottom = int(max(ys))\n",
        "\n",
        "    return [left, top, right, bottom]\n",
        "\n",
        "font_path = Path(cv2.__path__[0]) / \"qt/fonts/DejaVuSansCondensed.ttf\"\n",
        "font = ImageFont.truetype(str(font_path), size=12)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(28, 28))\n",
        "\n",
        "left_image = Image.open(image_path).convert(\"RGB\")\n",
        "right_image = Image.new(\"RGB\", left_image.size, (255, 255, 255))\n",
        "\n",
        "left_draw = ImageDraw.Draw(left_image)\n",
        "right_draw = ImageDraw.Draw(right_image)\n",
        "\n",
        "for i, (bbox, word, confidence) in enumerate(ocr_result):\n",
        "    box = create_bounding_box(bbox)\n",
        "\n",
        "    left_draw.rectangle(box, outline=\"blue\", width=2)\n",
        "    left, top, right, bottom = box\n",
        "\n",
        "    left_draw.text((right + 5, top), text=str(i + 1), fill=\"red\", font=font)\n",
        "    right_draw.text((left, top), text=word, fill=\"black\", font=font)\n",
        "\n",
        "ax1.imshow(left_image)\n",
        "ax2.imshow(right_image)\n",
        "ax1.axis(\"off\");\n",
        "ax2.axis(\"off\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uW2DqhK0NsjA"
      },
      "outputs": [],
      "source": [
        "for image_path in tqdm(image_paths):\n",
        "    ocr_result = reader.readtext(str(image_path), batch_size=16)\n",
        "\n",
        "    ocr_page = []\n",
        "    for bbox, word, confidence in ocr_result:\n",
        "        ocr_page.append({\n",
        "            \"word\": word, \"bounding_box\": create_bounding_box(bbox)\n",
        "        })\n",
        "\n",
        "    with image_path.with_suffix(\".json\").open(\"w\") as f:\n",
        "        json.dump(ocr_page, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2wDXECOQOo2"
      },
      "outputs": [],
      "source": [
        "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
        "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\n",
        "    \"microsoft/layoutlmv3-base\"\n",
        ")\n",
        "processor = LayoutLMv3Processor(feature_extractor, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdzU7yKZQjfg"
      },
      "outputs": [],
      "source": [
        "image_path = image_paths[0]\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "width, height = image.size\n",
        "\n",
        "width_scale = 1000 / width\n",
        "height_scale = 1000 / height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaZWMIYHQvuB"
      },
      "outputs": [],
      "source": [
        "def scale_bounding_box(box: List[int], width_scale : float = 1.0, height_scale : float = 1.0) -> List[int]:\n",
        "    return [\n",
        "        int(box[0] * width_scale),\n",
        "        int(box[1] * height_scale),\n",
        "        int(box[2] * width_scale),\n",
        "        int(box[3] * height_scale)\n",
        "    ]\n",
        "\n",
        "json_path = image_path.with_suffix(\".json\")\n",
        "with json_path.open(\"r\") as f:\n",
        "    ocr_result = json.load(f)\n",
        "\n",
        "words = []\n",
        "boxes = []\n",
        "for row in ocr_result:\n",
        "    boxes.append(scale_bounding_box(row[\"bounding_box\"], width_scale, height_scale))\n",
        "    words.append(row[\"word\"])\n",
        "\n",
        "len(words), len(boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUM10BAeQ1We"
      },
      "outputs": [],
      "source": [
        "encoding = processor(\n",
        "    image,\n",
        "    words,\n",
        "    boxes=boxes,\n",
        "    max_length=512,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"\"\"\n",
        "input_ids:  {list(encoding[\"input_ids\"].squeeze().shape)}\n",
        "word boxes: {list(encoding[\"bbox\"].squeeze().shape)}\n",
        "image data: {list(encoding[\"pixel_values\"].squeeze().shape)}\n",
        "image size: {image.size}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf2PQkEVQ48X"
      },
      "outputs": [],
      "source": [
        "image_data = encoding[\"pixel_values\"][0]\n",
        "transform = T.ToPILImage()\n",
        "transform(image_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzYI4MnnQ9iW"
      },
      "outputs": [],
      "source": [
        "model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/layoutlmv3-base\", num_labels=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO36pY1ZRLon"
      },
      "outputs": [],
      "source": [
        "outputs = model(**encoding)\n",
        "outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zfX8CqCRSoO"
      },
      "outputs": [],
      "source": [
        "train_images, test_images = train_test_split(image_paths, test_size=.2)\n",
        "DOCUMENT_CLASSES = sorted(list(map(\n",
        "    lambda p: p.name,\n",
        "    Path(\"images\").glob(\"*\")\n",
        ")))\n",
        "DOCUMENT_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP39UJQERV60"
      },
      "outputs": [],
      "source": [
        "class DocumentClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, processor):\n",
        "        self.image_paths = image_paths\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        image_path = self.image_paths[item]\n",
        "        json_path = image_path.with_suffix(\".json\")\n",
        "        with json_path.open(\"r\") as f:\n",
        "            ocr_result = json.load(f)\n",
        "\n",
        "            with Image.open(image_path).convert(\"RGB\") as image:\n",
        "\n",
        "                width, height = image.size\n",
        "                width_scale = 1000 / width\n",
        "                height_scale = 1000 / height\n",
        "\n",
        "                words = []\n",
        "                boxes = []\n",
        "                for row in ocr_result:\n",
        "                    boxes.append(scale_bounding_box(\n",
        "                        row[\"bounding_box\"],\n",
        "                        width_scale,\n",
        "                        height_scale\n",
        "                    ))\n",
        "                    words.append(row[\"word\"])\n",
        "\n",
        "                encoding = self.processor(\n",
        "                    image,\n",
        "                    words,\n",
        "                    boxes=boxes,\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "        label = DOCUMENT_CLASSES.index(image_path.parent.name)\n",
        "\n",
        "        return dict(\n",
        "            input_ids=encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
        "            bbox=encoding[\"bbox\"].flatten(end_dim=1),\n",
        "            pixel_values=encoding[\"pixel_values\"].flatten(end_dim=1),\n",
        "            labels=torch.tensor(label, dtype=torch.long)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr2_d_Q9Rdk6"
      },
      "outputs": [],
      "source": [
        "train_dataset = DocumentClassificationDataset(train_images, processor)\n",
        "test_dataset = DocumentClassificationDataset(test_images, processor)\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdT5zN8FRg42"
      },
      "outputs": [],
      "source": [
        "class ModelModule(pl.LightningModule):\n",
        "    def __init__(self, n_classes:int):\n",
        "        super().__init__()\n",
        "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
        "            \"microsoft/layoutlmv3-base\",\n",
        "            num_labels=n_classes\n",
        "        )\n",
        "        self.model.config.id2label = {k: v for k, v in enumerate(DOCUMENT_CLASSES)}\n",
        "        self.model.config.label2id = {v: k for k, v in enumerate(DOCUMENT_CLASSES)}\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            bbox=bbox,\n",
        "            pixel_values=pixel_values,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        bbox = batch[\"bbox\"]\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        output = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
        "        self.log(\"train_loss\", output.loss)\n",
        "        self.log(\n",
        "            \"train_acc\",\n",
        "            self.train_accuracy(output.logits, labels),\n",
        "            on_step=True,\n",
        "            on_epoch=True\n",
        "        )\n",
        "        return output.loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        bbox = batch[\"bbox\"]\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        output = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
        "        self.log(\"val_loss\", output.loss)\n",
        "        self.log(\n",
        "            \"val_acc\",\n",
        "            self.val_accuracy(output.logits, labels),\n",
        "            on_step=False,\n",
        "            on_epoch=True\n",
        "        )\n",
        "        return output.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.00001) #1e-5\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky2ub0sfRj5y"
      },
      "outputs": [],
      "source": [
        "model_module = ModelModule(len(DOCUMENT_CLASSES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLkOiS4ERl-C"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXnu4Cf1Rpty"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = ModelCheckpoint(\n",
        "    filename=\"{epoch}-{step}-{val_loss:.4f}\", save_last=True, save_top_k=3, monitor=\"val_loss\", mode=\"min\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    precision=16,\n",
        "    devices=1,\n",
        "    max_epochs=5,\n",
        "    callbacks=[\n",
        "        model_checkpoint\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0CZT9w5Rsq0"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model_module, train_data_loader, test_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTYVxqwmRvHC"
      },
      "outputs": [],
      "source": [
        "trained_model = ModelModule.load_from_checkpoint(\n",
        "    model_checkpoint.best_model_path,\n",
        "    n_classes=len(DOCUMENT_CLASSES),\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "trained_model.model.push_to_hub(\n",
        "    \"layoutlmv3-financial-document-classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcH6T-RSRx4d"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
        "    \"curiousily/layoutlmv3-financial-document-classification\"\n",
        ")\n",
        "model = model.eval().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3grBNUNR0DW"
      },
      "outputs": [],
      "source": [
        "def predict_document_image(\n",
        "    image_path: Path,\n",
        "    model: LayoutLMv3ForSequenceClassification,\n",
        "    processor: LayoutLMv3Processor):\n",
        "\n",
        "    json_path = image_path.with_suffix(\".json\")\n",
        "    with json_path.open(\"r\") as f:\n",
        "        ocr_result = json.load(f)\n",
        "\n",
        "        with Image.open(image_path).convert(\"RGB\") as image:\n",
        "\n",
        "            width, height = image.size\n",
        "            width_scale = 1000 / width\n",
        "            height_scale = 1000 / height\n",
        "\n",
        "            words = []\n",
        "            boxes = []\n",
        "            for row in ocr_result:\n",
        "                boxes.append(\n",
        "                    scale_bounding_box(\n",
        "                        row[\"bounding_box\"],\n",
        "                        width_scale,\n",
        "                        height_scale\n",
        "                    )\n",
        "                )\n",
        "                words.append(row[\"word\"])\n",
        "\n",
        "            encoding = processor(\n",
        "                image,\n",
        "                words,\n",
        "                boxes=boxes,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output = model(\n",
        "            input_ids=encoding[\"input_ids\"].to(DEVICE),\n",
        "            attention_mask=encoding[\"attention_mask\"].to(DEVICE),\n",
        "            bbox=encoding[\"bbox\"].to(DEVICE),\n",
        "            pixel_values=encoding[\"pixel_values\"].to(DEVICE)\n",
        "        )\n",
        "\n",
        "    predicted_class = output.logits.argmax()\n",
        "    return model.config.id2label[predicted_class.item()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDMF71doR2mm"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "predictions = []\n",
        "for image_path in tqdm(test_images):\n",
        "    labels.append(image_path.parent.name)\n",
        "    predictions.append(\n",
        "        predict_document_image(image_path, model, processor)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bZP0FR2R4rC"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(labels, predictions, labels=DOCUMENT_CLASSES)\n",
        "cm_display = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=DOCUMENT_CLASSES\n",
        ")\n",
        "\n",
        "cm_display.plot()\n",
        "cm_display.ax_.set_xticklabels(DOCUMENT_CLASSES, rotation=45)\n",
        "cm_display.figure_.set_size_inches(16, 8)\n",
        "\n",
        "plt.show();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
